Got it—here’s a tiny, **speed-focused Node.js + Redis caching app** you can run end-to-end. It shows a slow “DB” vs. Redis-cached responses and includes a quick benchmark.

---

# 1) Setup

```bash
mkdir redis-caching-speed && cd $_
npm init -y
npm i express redis
```

(Optional) run Redis via Docker:

```bash
docker run -d --name redis -p 6379:6379 redis:7-alpine
```

Create `.env` (optional):

```
REDIS_URL=redis://localhost:6379
PORT=3000
```

---

# 2) Project files

## `index.js`

```js
const express = require('express');
const { createClient } = require('redis');

const app = express();
app.use(express.json());

const PORT = process.env.PORT || 3000;
const redis = createClient({ url: process.env.REDIS_URL || 'redis://localhost:6379' });
redis.on('error', err => console.error('Redis error:', err));

(async () => {
  await redis.connect();
  console.log('✅ Connected to Redis');
})();

// --- Simulated slow DB ---
const fakeDb = {
  // pretend these are DB rows
  products: Array.from({ length: 50 }, (_, i) => ({ id: i + 1, name: `Product-${i + 1}`, price: (i + 1) * 3 })),
  async getProduct(id) {
    // 900ms latency to simulate DB/HTTP call
    await new Promise(r => setTimeout(r, 900));
    return this.products.find(p => p.id === Number(id)) || null;
  },
  async updateProduct(id, patch) {
    const idx = this.products.findIndex(p => p.id === Number(id));
    if (idx === -1) return null;
    this.products[idx] = { ...this.products[idx], ...patch };
    // pretend DB write latency
    await new Promise(r => setTimeout(r, 300));
    return this.products[idx];
  }
};

// --- Helper: cache key builders ---
const keyProduct = (id) => `product:${id}`;

// --- GET /products/:id with cache-aside (TTL=60s) ---
app.get('/products/:id', async (req, res) => {
  const id = req.params.id;
  const key = keyProduct(id);

  const t0 = Date.now();
  const cached = await redis.get(key);
  if (cached) {
    const ms = Date.now() - t0;
    return res.json({ source: 'cache', took_ms: ms, data: JSON.parse(cached) });
  }

  const data = await fakeDb.getProduct(id);
  if (!data) return res.status(404).json({ message: 'Not found' });

  await redis.set(key, JSON.stringify(data), { EX: 60 }); // TTL 60s
  const ms = Date.now() - t0;
  res.json({ source: 'db', took_ms: ms, data });
});

// --- PUT /products/:id to update + invalidate cache ---
app.put('/products/:id', async (req, res) => {
  const id = req.params.id;
  const updated = await fakeDb.updateProduct(id, req.body || {});
  if (!updated) return res.status(404).json({ message: 'Not found' });

  await redis.del(keyProduct(id)); // invalidate
  res.json({ message: 'updated & cache invalidated', data: updated });
});

// --- Health ---
app.get('/health', (_, res) => res.send('OK'));

app.listen(PORT, () => console.log(`🚀 http://localhost:${PORT}`));
```

## `package.json` (scripts snippet)

```json
{
  "name": "redis-caching-speed",
  "version": "1.0.0",
  "main": "index.js",
  "type": "commonjs",
  "scripts": {
    "dev": "node index.js"
  }
}
```

---

# 3) Run & Test

```bash
node index.js
# First hit: slow (db)
curl http://localhost:3000/products/7
# Second hit (within 60s): fast (cache)
curl http://localhost:3000/products/7
```

You’ll see responses like:

```json
{
  "source": "db",      // or "cache"
  "took_ms": 904,      // ~900ms first time, ~1-3ms from cache
  "data": { "id": 7, "name": "Product-7", "price": 21 }
}
```

Update + invalidate cache:

```bash
curl -X PUT http://localhost:3000/products/7 \
  -H "Content-Type: application/json" \
  -d '{"price": 199}'
# Next GET will be a DB miss again, then re-cached
curl http://localhost:3000/products/7
```

---

# 4) Quick Benchmark (optional)

Install autocannon:

```bash
npm i -g autocannon
```

Run two rounds:

1. Cold (forces DB once):

```bash
autocannon -d 10 -c 50 http://localhost:3000/products/25
```

2. Warm (cached):

```bash
autocannon -d 10 -c 50 http://localhost:3000/products/25
```

You should see **RPS much higher** and **latency much lower** in the cached round.

---

# 5) What this shows (Cache-Aside pattern)

* **Read path:**

  1. Try Redis → hit? return fast
  2. Miss → fetch DB (slow) → set Redis with TTL → return
* **Write path:**
  Update DB → **invalidate** Redis key → next read refreshes cache

---

If you want, I can turn this into a **NestJS** version (module + service + interceptor) with the same cache-aside strategy and a built-in rate-limit example.
